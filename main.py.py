# -*- coding: utf-8 -*-
"""Untitled14.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1r5lZpCnMt2AdmfAEx-yi6A-4CVaybRqo
"""

# your logistic regression
import pandas as pd
import numpy as np
from sklearn.metrics import accuracy_score
from scipy.special import softmax
import re
import string
import spacy
from spacy.lang.en import stop_words as spacy_stopwords
import numpy as np
from scipy import sparse
from numpy import array
from numpy import argmax
from scipy.sparse import csr_matrix
import matplotlib.pyplot as plt
from scipy.sparse import csr_matrix
import math
import operator
import numpy as np
from collections import Counter
from sklearn.preprocessing import normalize
import time
import pickle
import matplotlib.pyplot as plt
nlp = spacy.load("en")
df_train = pd.read_csv('train.csv')
df_test = pd.read_csv('test.csv')
df_traintf = df_train
df_testtf = df_test
df = pd.concat([df_train, df_test])
temp = pd.concat([df_traintf, df_testtf])


def IDF(corpus, unique_words):
    idf_dict = {}
    N = len(corpus)
    for i in unique_words:
        count = 0
        for sen in corpus:
            if i in sen.split():
                count = count + 1
            idf_dict[i] = (math.log((1 + N) / (count + 1))) + 1
    return idf_dict


def fit(whole_data):
    unique_words = set()
    if isinstance(whole_data, (list,)):
        for x in whole_data:
            for y in x.split():
                if len(y) < 2:
                    continue
                unique_words.add(y)
        unique_words = sorted(list(unique_words))
        vocab = {j: i for i, j in enumerate(unique_words)}
        Idf_values_of_all_unique_words = IDF(whole_data, unique_words)
    return vocab, Idf_values_of_all_unique_words


Vocabulary, idf_of_vocabulary = fit(list(temp['text']))

#
# def transform(dataset, vocabulary, idf_values):
#     sparse_matrix = csr_matrix((len(dataset), len(vocabulary)), dtype=np.float64)
#     for row in range(0, len(dataset)):
#         number_of_words_in_sentence = Counter(dataset[row].split())
#         for word in dataset[row].split():
#             if word in list(vocabulary.keys()):
#                 tf_idf_value = (number_of_words_in_sentence[word] / len(dataset[row].split())) * (idf_values[word])
#                 sparse_matrix[row, vocabulary[word]] = tf_idf_value
#     output = normalize(sparse_matrix, norm='l2', axis=1, copy=True, return_norm=False)
#     return output
#
#
# final_output = transform(list(temp['text']), Vocabulary, idf_of_vocabulary)
# temp['emotions'] = temp['emotions'].astype('category').cat.codes
#
# tfidf = pd.DataFrame.sparse.from_spmatrix(final_output)
# df_train['label'] = df_train['emotions'].astype('category').cat.codes
# tfidf = tfidf.join(df_train['label'])
# print(tfidf)
#
# maximum = np.nanmax(tfidf[tfidf.columns[:-1]].iloc[0:1200].values)
# minimum = np.nanmin(tfidf[tfidf.columns[:-1]].iloc[0:1200].values)
# print(minimum)
#
#
# def data_normalization(data, minimum, maximum):
#     data = ((data - minimum) / (maximum - minimum))
#     return data
#
#
# tfidf[tfidf.columns[:-1]] = data_normalization(tfidf[tfidf.columns[:-1]], minimum, maximum)
# tfidf.to_csv("tfidf.csv")
""" **Method for cross validating**
    """
"""**Count Vectorizer from scratch**"""


class CountVectorizer():
    def __init__(self, pass_stop=True):
        self.pass_stop = pass_stop

    def fit(self, data):
        data = map(lambda x: str(x).split(" "), data)
        self.elements_ = set()
        for line in data:
            for x in line:
                if self.pass_stop:
                    if len(x) == 1:
                        continue
                self.elements_.add(x)
        self.elements_ = np.sort(list(self.elements_))
        self.labels_ = np.arange(len(self.elements_)).astype(int)
        self.dict_ = {}
        for i in range(len(self.elements_)):
            self.dict_[str(self.elements_[i])] = self.labels_[i]

    def transform(self, data):
        rows = []
        cols = []
        data = list(map(lambda x: str(x).split(" "), data))
        for i in range(len(data)):
            for x in data[i]:
                if self.pass_stop:
                    if len(x) == 1:
                        continue
                rows.append(i)
                cols.append(self.dict_[x])
        vals = np.ones((len(rows),)).astype(int)

        return sparse.csr_matrix((vals, (rows, cols)), shape=(len(data), len(self.labels_)))


def preprocess(text):
    # text = preprocessor.clean(text)
    text = re.sub(r'\W+', ' ', text)  # remove non-alphanumeric characters
    # replace numbers with the word 'number'
    text = re.sub(r"\d+", "number", text)
    text = text.lower()  # lower case everything

    return text.strip()  # remove redundant spaces


df_train['text'] = df_train['text'].apply(preprocess)
df_train['emotions'] = df_train['emotions'].astype('category').cat.codes

df_test['text'] = df_test['text'].apply(preprocess)

integer_encoded = df_train['emotions']


vectorizer = CountVectorizer()

A=(pd.concat([df_test['text'],df_train['text']]))
print(type(A))
vectorizer.fit(A)
print(len(A))
vector = vectorizer.transform(A)
print(vector.shape)

df = pd.DataFrame(data=(vector.toarray()))
df['label']=df_train['emotions']
df_test1=df.iloc[1200:]
df_train=df.iloc[0:1200]
df_test1=df.iloc[1200:]

df_test1=df.iloc[1200:]
df_train=df.iloc[0:1200]
df_test1=df.iloc[1200:]
def loss(X, Y, W):
    """
    Y: onehot encoded
    """
    Z = - X @ W
    N = X.shape[0]
    loss = 1 / N * (np.trace(X @ W @ Y.T) + np.sum(np.log(np.sum(np.exp(Z), axis=1))))
    return loss


def gradient(X, Y, W, mu):
    """
    Y: onehot encoded
    """
    Z = - X @ W
    P = softmax(Z, axis=1)
    N = X.shape[0]
    gd = 1 / N * (X.T @ (Y - P)) + 2 * mu * W
    return gd


def gradient_descent(X, Y, max_iter=100, eta=0.5, mu=0.01):
    """
    Very basic gradient descent algorithm with fixed eta and mu
    """

    def oneHotIt(Y):
        m = Y.shape[0]
        y_hot = csr_matrix((np.ones(m), (Y, np.array(range(m)))))
        y_hot = np.array(y_hot.todense()).T
        return y_hot

    Y_onehot = oneHotIt(Y)
    W = np.zeros((X.shape[1], Y_onehot.shape[1]))
    step = 0
    step_lst = []
    loss_lst = []
    W_lst = []

    while step < max_iter:
        step += 1
        W -= eta * gradient(X, Y_onehot, W, mu)
        step_lst.append(step)
        W_lst.append(W)
        loss_lst.append(loss(X, Y_onehot, W))

    df = pd.DataFrame({
        'step': step_lst,
        'loss': loss_lst
    })
    return df, W


class Multiclass:
    def fit(self, X, Y):
        self.loss_steps, self.W = gradient_descent(X, Y)

    def loss_plot(self):
        return self.loss_steps.plot(
            x='step',
            y='loss',
            xlabel='step',
            ylabel='loss'
        )

    def predict1(self, H):
        Z = - H @ self.W
        P = softmax(Z, axis=1)
        return np.argmax(np.array(P), axis=1)


"""**Loading Text** and preprocessing it by vectorzing the text"""
class LR():
        S1 = tfidf.values[0:300, :]
        S2 = tfidf.values[300:600, :]
        S3 = tfidf.values[600:900, :]
        S4 = tfidf.values[900:1200, :]

        def union_trainset(j, S1, S2, S3, S4):
            train_set_union = []
            if j == 1:
                test_set = pd.DataFrame(S1)
                train_set_union = np.concatenate((S2, S3, S4), axis=0)
                train_set_union_df = pd.DataFrame(train_set_union)
            if j == 2:
                test_set = pd.DataFrame(S2)
                train_set_union = np.concatenate((S1, S3, S4), axis=0)
                train_set_union_df = pd.DataFrame(train_set_union)
            if j == 3:
                test_set = pd.DataFrame(S3)
                train_set_union = np.concatenate((S1, S2, S4), axis=0)
                train_set_union_df = pd.DataFrame(train_set_union)
            if j == 4:
                test_set = pd.DataFrame(S4)
                train_set_union = np.concatenate((S1, S2, S3), axis=0)
                train_set_union_df = pd.DataFrame(train_set_union)

            return train_set_union_df, test_set
        lrtf = Multiclass()
        lrtf.fit(tfidf[tfidf.columns[:-1]].iloc[0:1200],df_train['label'].astype(int))
        t=lrtf.predict1(tfidf[tfidf.columns[:-1]].iloc[1200:])  #df_test=test.csv
        df_test['labels']=(t)
        (df_test).to_csv("test_lg.csv")
        s = pd.read_csv('test_lg.csv')
        s['labels'].replace({5: 'surprise', 4: 'sadness', 3: 'love', 2: 'joy', 1: 'fear', 0: 'anger'}, inplace=True)
        s.to_csv('test_lg.csv')
        ######cross validation######
        # listofacc = []
        # trainset, testset = union_trainset(4, S1, S2, S3, S4)
        # lrtf1 = Multiclass()
        # lrtf1.fit(trainset[trainset.columns[:-1]], trainset[trainset.columns[-1]].astype(int))
        # p = lrtf1.predict1((testset[testset.columns[:-1]]))
        # acc = (accuracy_score((testset[testset.columns[-1]]).astype(int), p))
        # listofacc.append(acc)
        # trainset, testset = union_trainset(3, S1, S2, S3, S4)
        # lrtf1.fit(trainset[trainset.columns[:-1]], trainset[trainset.columns[-1]].astype(int))
        # p = lrtf1.predict1((testset[testset.columns[:-1]]))
        # acc = (accuracy_score((testset[testset.columns[-1]]).astype(int), p))
        # listofacc.append(acc)
        # trainset, testset = union_trainset(2, S1, S2, S3, S4)
        # lrtf1.fit(trainset[trainset.columns[:-1]], trainset[trainset.columns[-1]].astype(int))
        # p = lrtf1.predict1((testset[testset.columns[:-1]]))
        # acc = (accuracy_score((testset[testset.columns[-1]]).astype(int), p))
        # listofacc.append(acc)
        # trainset, testset = union_trainset(1, S1, S2, S3, S4)
        # lrtf1.fit(trainset[trainset.columns[:-1]], trainset[trainset.columns[-1]].astype(int))
        # p = lrtf1.predict1((testset[testset.columns[:-1]]))
        # acc = (accuracy_score((testset[testset.columns[-1]]).astype(int), p))
        # listofacc.append(acc)
        # plt.plot(listofacc)
        # plt.show()


class DeepNeuralNetwork():
    def __init__(self, sizes, epochs=5, l_rate=0.5):
        self.sizes = sizes
        self.epochs = epochs
        self.l_rate = l_rate

        # we save all parameters in the neural network in this dictionary
        self.params = self.initialization()

    def sigmoid(self, x, derivative=False):
        if derivative:
            return (np.exp(-x)) / ((np.exp(-x) + 1) ** 2)
        return 1 / (1 + np.exp(-x))

    def softmax(self, x, derivative=False):
        # Numerically stable with large exponentials
        exps = np.exp(x - x.max())
        if derivative:
            return exps / np.sum(exps, axis=0) * (1 - exps / np.sum(exps, axis=0))
        return exps / np.sum(exps, axis=0)

    def initialization(self):
        # number of nodes in each layer
        input_layer = self.sizes[0]
        hidden_1 = self.sizes[1]
        hidden_2 = self.sizes[2]
        hidden_3 = self.sizes[3]
        hidden_4 = self.sizes[4]
        output_layer = self.sizes[5]

        params = {
            'W1': np.random.randn(hidden_1, input_layer) * np.sqrt(1. / hidden_1),
            'W2': np.random.randn(hidden_2, hidden_1) * np.sqrt(1. / hidden_2),

            'W3': np.random.randn(hidden_3, hidden_2) * np.sqrt(1. / hidden_3),
            'W4': np.random.randn(hidden_4, hidden_3) * np.sqrt(1. / hidden_4),

            'W5': np.random.randn(output_layer, hidden_4) * np.sqrt(1. / output_layer)
        }

        return params

    def forward_pass(self, x_train):
        params = self.params

        # input layer activations becomes sample
        params['A0'] = x_train

        # input layer to hidden layer 1
        params['Z1'] = np.dot(params["W1"], params['A0'])
        params['A1'] = self.sigmoid(params['Z1'])

        # hidden layer 1 to hidden layer 2
        params['Z2'] = np.dot(params["W2"], params['A1'])
        params['A2'] = self.sigmoid(params['Z2'])

        params['Z3'] = np.dot(params["W3"], params['A2'])
        params['A3'] = self.sigmoid(params['Z3'])

        params['Z4'] = np.dot(params["W4"], params['A3'])
        params['A4'] = self.sigmoid(params['Z4'])

        # hidden layer 2 to output layer
        params['Z5'] = np.dot(params["W5"], params['A4'])
        params['A5'] = self.softmax(params['Z5'])

        return params['A5'], params['Z5']

    def backward_pass(self, y_train, output):

        params = self.params
        change_w = {}

        # Calculate W3 update
        error = 2 * (output - y_train) / output.shape[0] * self.softmax(params['Z5'], derivative=True)
        change_w['W5'] = np.outer(error, params['A4'])

        # Calculate W2 update
        error = np.dot(params['W5'].T, error) * self.sigmoid(params['Z4'], derivative=True)
        change_w['W4'] = np.outer(error, params['A3'])

        # Calculate W2 update
        error = np.dot(params['W4'].T, error) * self.sigmoid(params['Z3'], derivative=True)
        change_w['W3'] = np.outer(error, params['A2'])

        # Calculate W2 update
        error = np.dot(params['W3'].T, error) * self.sigmoid(params['Z2'], derivative=True)
        change_w['W2'] = np.outer(error, params['A1'])

        # Calculate W1 update
        error = np.dot(params['W2'].T, error) * self.sigmoid(params['Z1'], derivative=True)
        change_w['W1'] = np.outer(error, params['A0'])
        return change_w

    def update_network_parameters(self, changes_to_w):
        '''
            Update network parameters according to update rule from
            Stochastic Gradient Descent.

            θ = θ - η * ∇J(x, y),
                theta θ:            a network parameter (e.g. a weight w)
                eta η:              the learning rate
                gradient ∇J(x, y):  the gradient of the objective function,
                                    i.e. the change for a specific theta θ
        '''

        for key, value in changes_to_w.items():
            self.params[key] -= self.l_rate * value

    def compute_accuracy(self, x_val, y_val):
        '''
            This function does a forward pass of x, then checks if the indices
            of the maximum value in the output equals the indices in the label
            y. Then it sums over each prediction and calculates the accuracy.
        '''
        predictions = []
        count = 0
        for x, y in zip(x_val, y_val):
            output, _ = self.forward_pass(x)
            pred = np.argmax(output)
            predictions.append(pred == np.argmax(y))
            if (pred == y):
                count = count + 1
        accuracy = count / (float(len(y_val)))
        return accuracy

    def train(self, x_train, y_train, x_val, y_val, boolval):
        start_time = time.time()
        for iteration in range(self.epochs):
            for x, y in zip(x_train, y_train):
                output, _ = self.forward_pass(x)
                changes_to_w = self.backward_pass(y, output)
                self.update_network_parameters(changes_to_w)
            if (boolval is True):
                accuracy = self.compute_accuracy(x_val, y_val)
                print('Epoch: {0}, Time Spent: {1:.2f}s, Accuracy: {2:.2f}%'.format(
                    iteration + 1, time.time() - start_time, accuracy * 100
                ))





def NN():
    S1 = tfidf.values[0:300, :]
    S2 = tfidf.values[300:600, :]
    S3 = tfidf.values[600:900, :]
    S4 = tfidf.values[900:1200, :]

    def union_trainset(j, S1, S2, S3, S4):
        train_set_union = []
        if j == 1:
            test_set = pd.DataFrame(S1)
            train_set_union = np.concatenate((S2, S3, S4), axis=0)
            train_set_union_df = pd.DataFrame(train_set_union)
        if j == 2:
            test_set = pd.DataFrame(S2)
            train_set_union = np.concatenate((S1, S3, S4), axis=0)
            train_set_union_df = pd.DataFrame(train_set_union)
        if j == 3:
            test_set = pd.DataFrame(S3)
            train_set_union = np.concatenate((S1, S2, S4), axis=0)
            train_set_union_df = pd.DataFrame(train_set_union)
        if j == 4:
            test_set = pd.DataFrame(S4)
            train_set_union = np.concatenate((S1, S2, S3), axis=0)
            train_set_union_df = pd.DataFrame(train_set_union)

        return train_set_union_df, test_set

    # dnn = DeepNeuralNetwork(sizes=[4879, 2400, 1200, 300, 90, 6])
    # trainset, testset = union_trainset(4, S1, S2, S3, S4)
    # dnn.train(trainset[trainset.columns[:-1]].to_numpy(), trainset[trainset.columns[-1]].to_numpy(),
    #           testset[testset.columns[:-1]].to_numpy(), testset[testset.columns[-1]].to_numpy(), True)
    # trainset, testset = union_trainset(3, S1, S2, S3, S4)
    # dnn.train(trainset[trainset.columns[:-1]].to_numpy(), trainset[trainset.columns[-1]].to_numpy(),
    #           testset[testset.columns[:-1]].to_numpy(), testset[testset.columns[-1]].to_numpy(), True)
    # trainset, testset = union_trainset(2, S1, S2, S3, S4)
    # dnn.train(trainset[trainset.columns[:-1]].to_numpy(), trainset[trainset.columns[-1]].to_numpy(),
    #           testset[testset.columns[:-1]].to_numpy(), testset[testset.columns[-1]].to_numpy(), True)
    # trainset, testset = union_trainset(1, S1, S2, S3, S4)
    # dnn.train(trainset[trainset.columns[:-1]].to_numpy(), trainset[trainset.columns[-1]].to_numpy(),
    #           testset[testset.columns[:-1]].to_numpy(), testset[testset.columns[-1]].to_numpy(), True)
    # _, output_test = dnn.forward_pass((df_test1[df_test1.columns[:-1]].T))
    # output_test = np.argmax(output_test, axis=0)
    # print(output_test)
    # df_test['label'] = output_test
    #
    # # weights is a Python array
    # pickle.dump(_, open('weightswithcrossval.pkl', 'wb'))

    dnnfull = DeepNeuralNetwork(sizes=[4879, 2400, 1200, 300, 90, 6])
    dnnfull.train(tfidf[tfidf.columns[:-1]].iloc[0:1200].to_numpy(), tfidf[tfidf.columns[-1]].iloc[0:1200].to_numpy(),
                  0, 0,
                  False)

    # # weights is a Python array
    _, output_test = dnnfull.forward_pass(tfidf[tfidf.columns[:-1]].iloc[1200:].to_numpy().T)
    pickle.dump(_, open('weightsfullwithoutcrossval.pkl', 'wb'))
    output_test = np.argmax(output_test, axis=0)
    print(output_test)
    df_test['label']=output_test
    df_test.to_csv("test_nn.csv")
    s = pd.read_csv('test_nn.csv')
    s['label'].replace({5: 'surprise', 4: 'sadness', 3: 'love', 2: 'joy', 1: 'fear', 0: 'anger'}, inplace=True)
    s.to_csv('test_nn.csv')

    # your Multi-layer Neural Network

if __name__ == '__main__':
    print ("..................Beginning of Logistic Regression................")
    LR()
    print ("..................End of Logistic Regression................")

    print("------------------------------------------------")

    print ("..................Beginning of Neural Network................")
    NN()
    print ("..................End of Neural Network................")